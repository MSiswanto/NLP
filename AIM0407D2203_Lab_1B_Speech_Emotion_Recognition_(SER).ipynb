{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSiswanto/NLP/blob/main/AIM0407D2203_Lab_1B_Speech_Emotion_Recognition_(SER).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIM0407D2203 Lab 1B Speech Emotion Recognition (SER)\n",
        "---\n",
        "\n",
        "1. Ucapan dan emosi adalah dua aspek komunikasi terpenting di antara manusia yang menjadikan Speech Emotion Recognition (SER) sebagai salah satu komponen kunci pada sistem Human-Computer Interaction (HCI). \n",
        "2. SER adalah salah satu tugas NLP untuk mengenali aspek emosional dari ucapan manusia terlepas dari isi semantiknya.\n",
        "3. SER dapat dikembangkan untuk banyak aplikasi, misalnya untuk aplikasi bagi individu dengan Gangguan Spektrum Autisme.\n",
        "\n",
        "Tujuan utama dari eksperimen ini adalah untuk membuat sistem SER sederhana dan mengeksplorasi algoritma machine learning yang berbeda untuk tugas SER. Secara khusus, kita akan membandingkan pendekatan statistik tradisional dengan metode deep learning yang lebih modern berdasarkan metrik evaluasi untuk mempelajari lebih lanjut tentang struktur data dan kompleksitas tugas SER.\n",
        "\n",
        "Secara umum, skema yang akan kita lakukan umtuk eksperimen ini akan mengikuti bagan berikut:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "107pbuWx8wWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01 Install & Import Library"
      ],
      "metadata": {
        "id": "zJaBd4Ck9NaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Import the audio playback widget\n",
        "import IPython.display as ipd"
      ],
      "metadata": {
        "id": "uFgcGpMF9M5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 Data Acquisition\n",
        "Kita akan menggunakan RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset untuk melakukan eksperimen ini. Dataset terdiri dari 1440 file dari 24 aktor profesional (12 pria dan 12 wanita) yang dibagi dalam 8 ekspresi emosi: calm, neutral, surprised, happy, angry, sad, fearful, or disgust. Setiap aktor menyuarakan dua pernyataan: 01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\".\n",
        "\n",
        "Baca selanjutnya tentang RAVDESS: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196391"
      ],
      "metadata": {
        "id": "oaXmsQaN8hW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/u/1/uc?id=1sX82joy1Rcw9p-YHZzc0bEfqfzOzckzY&export=download\n",
        "!gdown 1sX82joy1Rcw9p-YHZzc0bEfqfzOzckzY"
      ],
      "metadata": {
        "id": "C89zhU1C7h3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q 'AIM0407D2203 Lab 1B RAVDESS Speech dataset.zip'"
      ],
      "metadata": {
        "id": "BW4NAn0F8ODc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "5_05D11t77RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAVD_PATH = 'ravdess_speech_dataset/'   # Path to the directory\n",
        "\n",
        "dir_list = os.listdir(RAVD_PATH)        # List files in audio directory\n",
        "dir_list.sort()\n",
        "\n",
        "dir_list"
      ],
      "metadata": {
        "id": "0ZdomTjH9ab4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion = []\n",
        "gender = []\n",
        "actor = []\n",
        "file_path = []\n",
        "\n",
        "for i in dir_list:\n",
        "  filename = os.listdir(RAVD_PATH + i)    # Iterate over actor folders\n",
        "  for f in filename:                      # Go through files in Actor folder\n",
        "    part = f.split('.')[0].split('-')\n",
        "    emotion.append(int(part[2]))\n",
        "    actor.append(int(part[6]))\n",
        "    temp = int(part[6])\n",
        "    if temp%2 == 0:\n",
        "      temp = 'female'\n",
        "    else:\n",
        "      temp = 'male'\n",
        "    gender.append(temp)\n",
        "    file_path.append(RAVD_PATH + i + '/' + f)"
      ],
      "metadata": {
        "id": "ET3ZoUkO9zq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(emotion)\n",
        "df = df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
        "df = pd.concat([pd.DataFrame(gender), df, pd.DataFrame(actor)], axis=1)\n",
        "df.columns = ['gender','emotion','actor']\n",
        "df = pd.concat([df, pd.DataFrame(file_path, columns=['path'])], axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "--76OBdF_OYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.emotion.value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "lxa3MibaBpj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('speech_path_df.csv')"
      ],
      "metadata": {
        "id": "Sppud8Z9CGHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03 Exploratory Data Analysis (EDA)\n",
        "\n",
        "Fitur utama dari data audio adalah MFCC (Mel Frequency Cepstral Coefficients) dan Mel Spectrogram\n",
        "\n",
        "1. MFCC (Mel Frequency Cepstral Coefficients) - MFCC adalah ekstraksi fitur penting saat menggunakan data ucapan. Skala Mel adalah skala yang menghubungkan frekuensi nada yang dirasakan dengan frekuensi nyata yang diukur.\n",
        "2. Mel Spectogram - A Fast Fourier Transform dihitung pada segmen berjendela yang tumpang tindih dari sinyal. Spektogram adalah cara visual representasi kekuatan sinyal dan juga digunakan untuk menampilkan gelombang frekuensi suara."
      ],
      "metadata": {
        "id": "xo6J6ODX8gVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Male Angry"
      ],
      "metadata": {
        "id": "6mWQV6uAEIM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_angry = RAVD_PATH + 'Actor_03/03-01-05-01-01-01-03.wav'\n",
        "data, sr = librosa.load(male_angry)\n",
        "ipd.Audio(male_angry) "
      ],
      "metadata": {
        "id": "FrjYWXbCCT4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display waveplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "librosa.display.waveplot(data, sr=sr)\n",
        "plt.title('Waveplot - Male Angry')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h7gLQWtLFx55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Log Mel Spectogram\n",
        "spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128, fmax=8000) \n",
        "spectrogram = librosa.power_to_db(spectrogram)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "librosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\n",
        "plt.title('Mel Spectrogram - Male Angry')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Prjzqz9dGC7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display MFCC\n",
        "mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "plt.subplot(3,1,1)\n",
        "librosa.display.specshow(mfcc, x_axis='time')\n",
        "plt.title('MFCC - Male Angry')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2-OKY93HHeP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Female Angry"
      ],
      "metadata": {
        "id": "f_-Cl554JQlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_angry = RAVD_PATH + 'Actor_18/03-01-05-01-01-01-18.wav'\n",
        "data, sr = librosa.load(female_angry)\n",
        "ipd.Audio(female_angry) "
      ],
      "metadata": {
        "id": "9h-vqixPIVG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display waveplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "librosa.display.waveplot(data, sr=sr)\n",
        "plt.title('Waveplot - Female Angry')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N_e30Bt9JcVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Log Mel Spectogram\n",
        "spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128, fmax=8000) \n",
        "spectrogram = librosa.power_to_db(spectrogram)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "librosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\n",
        "plt.title('Mel Spectrogram - Female Angry')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G2xSA5-kJt5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display MFCC\n",
        "mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "plt.subplot(3,1,1)\n",
        "librosa.display.specshow(mfcc, x_axis='time')\n",
        "plt.title('MFCC - Female Angry')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8VEN1ZmFJxll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Male Angry vs Female Angry"
      ],
      "metadata": {
        "id": "TF3Rozl4LMn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gender - Male; Emotion - Angry\n",
        "male_angry = RAVD_PATH + 'Actor_03/03-01-05-01-01-01-03.wav'\n",
        "X, sample_rate = librosa.load(male_angry)  \n",
        "female = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
        "female = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
        "\n",
        "# Gender - Female; Emotion - Angry\n",
        "female_angry = RAVD_PATH + 'Actor_18/03-01-05-01-01-01-18.wav'\n",
        "X, sample_rate = librosa.load(female_angry)  \n",
        "male = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
        "male = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
        "\n",
        "# Plot the two audio waves together\n",
        "plt.figure(figsize=(16,10))\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(female, label='female')\n",
        "plt.plot(male, label='male')\n",
        "plt.title('Audio Waves - Male vs Female Angry')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mHuqiHgoK-YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04 Data Augmentation\n",
        "- Augmentasi data adalah proses di mana kita membuat sampel data terpolimerisasi baru dengan menambahkan sedikit gangguan pada set pelatihan awal.\n",
        "- Tujuannya adalah untuk membuat model kita invarian terhadap gangguan tersebut dan meningkatkan kemampuannya untuk menggeneralisasi. Agar ini berfungsi, menambahkan gangguan harus mempertahankan label yang sama dengan sampel pelatihan asli."
      ],
      "metadata": {
        "id": "oD6CApcHNQ-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil satu sample data yang diambil dari dataset\n",
        "sample_data = np.array(df['path'])[471]\n",
        "data, sample_rate = librosa.load(sample_data)"
      ],
      "metadata": {
        "id": "5IWHRPgIOHFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Audio"
      ],
      "metadata": {
        "id": "himIrvGvN8CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "librosa.display.waveshow(y=data, sr=sample_rate)\n",
        "plt.title('Normal Audio Sample')\n",
        "\n",
        "ipd.Audio(sample_data)"
      ],
      "metadata": {
        "id": "d1G66RfBN9GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio with Noise"
      ],
      "metadata": {
        "id": "T5agRXYTPfJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noise(data):\n",
        "  noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "  data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "  return data\n",
        "\n",
        "x = get_noise(data)\n",
        "plt.figure(figsize=(10,5))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "plt.title('Noise Audio Sample')\n",
        "\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "LNWgMOWUPXw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio with Pitch"
      ],
      "metadata": {
        "id": "wDq_o-FuRcJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "  return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
        "\n",
        "x = get_pitch(data, sample_rate)\n",
        "plt.figure(figsize=(10,5))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "plt.title('Noise Audio Pitch')\n",
        "\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "5vlTTqeTRfJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shifted Audio"
      ],
      "metadata": {
        "id": "9lZLzOjXQd_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_shift(data):\n",
        "  shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "  return np.roll(data, shift_range)\n",
        "\n",
        "x = get_shift(data)\n",
        "plt.figure(figsize=(10,5))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "plt.title('Shifted Audio Sample')\n",
        "\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "NY-OqSwUP1mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stretched Audio"
      ],
      "metadata": {
        "id": "DzXg6rLmRBpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stretch(data, rate=0.8):\n",
        "  return librosa.effects.time_stretch(data, rate)\n",
        "\n",
        "x = get_stretch(data)\n",
        "plt.figure(figsize=(10,5))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "plt.title('Stretched Audio Sample')\n",
        "\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "gjxHKShPQrQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Increase Audio Speed with Pitch"
      ],
      "metadata": {
        "id": "Pm0qDNrJT_WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_speedpitch(data):\n",
        "  length_change = np.random.uniform(low=0.8, high=1)\n",
        "  speed_fac = 1.4  / length_change \n",
        "  tmp = np.interp(np.arange(0, len(data),speed_fac), np.arange(0,len(data)), data)\n",
        "  minlen = min(data.shape[0], tmp.shape[0])\n",
        "  data *= 0\n",
        "  data[0:minlen] = tmp[0:minlen]\n",
        "  return data\n",
        "\n",
        "x = get_speedpitch(data)\n",
        "plt.figure(figsize=(10,5))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "plt.title('Stretched Audio Sample')\n",
        "\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "RTgz1G3hUReC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05 Feature Extraction\n",
        "\n",
        "Representasi domain suara sangat kompleks dan dalam bentuk aslinya, tidak memberikan wawasan yang sangat baik tentang karakteristik utama sinyal suara. Karena karakteristik sinyal suara ini, kami memetakan representasi domain waktu ini menjadi fitur yang lebih jelas. Teknik yang paling mudah melibatkan penentuan energi rata-rata sinyal. Metrik ini, bersama dengan energi total dalam sinyal, menunjukkan \"volume\" speaker. Durasi juga menawarkan wawasan tentang emosi, seperti halnya statistik seperti maksimum, minimum, jangkauan, rata-rata, dan standar deviasi dari sinyal dan spektrum. Ini mungkin menunjukkan fluktuasi dalam volume atau nada yang dapat berguna dalam menentukan emosi. Untuk sinyal dan spektrum, kami juga memperoleh kemiringan, ukuran penyimpangan simetri horizontal dalam sinyal, dan kurtosis, ukuran tinggi dan ketajaman puncak pusat, relatif terhadap kurva lonceng standar."
      ],
      "metadata": {
        "id": "vNuTYwSER7LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_audio_features(path):\n",
        "  data, sample_rate = librosa.load(path, res_type='kaiser_fast', sr=20000*2, duration=2.5, offset=0.5)\n",
        "  sampe_rate = np.array(sample_rate)\n",
        "                        \n",
        "  y_harmonic, y_percussive = librosa.effects.hpss(data)\n",
        "  pitches, magnitudes = librosa.core.pitch.piptrack(y=data, sr=sample_rate)\n",
        "\n",
        "  mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13), axis=1)\n",
        "  pitches = np.trim_zeros(np.mean(pitches, axis=1))[:20]\n",
        "  magnitudes = np.trim_zeros(np.mean(magnitudes, axis=1))[:20]\n",
        "  C = np.mean(librosa.feature.chroma_cqt(y=y_harmonic, sr=20000), axis=1)\n",
        "\n",
        "  return [mfcc, pitches, magnitudes, C]\n",
        "  \n",
        "def get_features_dataframe(dataframe):\n",
        "  features  = pd.DataFrame(columns=['mfcc','pitches','magnitudes','C'])\n",
        "  for index, audio_path in enumerate(df['path']):\n",
        "    features.loc[index] = get_audio_features(audio_path)\n",
        "  \n",
        "  mfcc = features.mfcc.apply(pd.Series)\n",
        "  pit = features.pitches.apply(pd.Series)\n",
        "  mag = features.magnitudes.apply(pd.Series)\n",
        "  C = features.C.apply(pd.Series)\n",
        "  \n",
        "  features = pd.concat([mfcc, pit, mag,C], axis=1, ignore_index=True)\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "Kcucea9PWhpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "features_df = pd.concat([df, get_features_dataframe(df)], axis=1)\n",
        "\n",
        "features_df"
      ],
      "metadata": {
        "id": "ARitYc-9Zvlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save features dataframe\n",
        "features_df.to_csv('speech_feature_df.csv', index=False)"
      ],
      "metadata": {
        "id": "Y9zO43NNpBHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06 Prepping Data for Modeling"
      ],
      "metadata": {
        "id": "6W23V-pYp_Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pisahkan kolom fitur dan target\n",
        "X = features_df.iloc[:, 4:].values\n",
        "y = features_df['emotion'].values"
      ],
      "metadata": {
        "id": "Epdhl5kZqv2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot Encoding"
      ],
      "metadata": {
        "id": "dYY7Jgb932Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features as a one-hot numeric array.\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "label_encoder = OneHotEncoder()\n",
        "y = label_encoder.fit_transform(np.array(y).reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "7HwEkAyDv6gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "VECQPQQ33hza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.categories_"
      ],
      "metadata": {
        "id": "0-fRf0pK54PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Test Data Splitting"
      ],
      "metadata": {
        "id": "Z1CnVvU-3y_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split arrays or matrices into random train and test subsets.\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=features_df[['emotion','gender','actor']], random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "vob8EAhPqE1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshape Data to 3D Tensor"
      ],
      "metadata": {
        "id": "P7-_hWFk6dwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = X_train.reshape(X_train.shape[0] , X_train.shape[1] , 1)\n",
        "X_test_tensor = X_test.reshape(X_test.shape[0] , X_test.shape[1] , 1)\n",
        "\n",
        "X_train_tensor.shape, X_test_tensor.shape"
      ],
      "metadata": {
        "id": "ZMwHc6tkvqbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 07 Modelling\n",
        "Kita akan membandingkan pendekatan statistik tradisional (baseline) dengan metode deep learning (CNN) untuk tugas SER."
      ],
      "metadata": {
        "id": "kns3EHzHrFCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model (Machine Learning)"
      ],
      "metadata": {
        "id": "Y99WQKS8uEK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree"
      ],
      "metadata": {
        "id": "iPBYjPJk71Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A decision tree classifier.\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf_1 = DecisionTreeClassifier()              \n",
        "\n",
        "clf_1.fit(X_train, y_train)\n",
        "clf_1.predict(X_test)\n",
        "\n",
        "print(\"Training set score: {:.3f}\".format(clf_1.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.3f}\".format(clf_1.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "MCsYPljHrbmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k Nearest Neighbor\n",
        "\n"
      ],
      "metadata": {
        "id": "D_Tih7aH-Vdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier implementing the k-nearest neighbors vote.\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf_2 = KNeighborsClassifier(n_neighbors=4)\n",
        "            \n",
        "clf_2.fit(X_train, y_train)\n",
        "clf_2.predict(X_test)\n",
        "\n",
        "print(\"Training set score: {:.3f}\".format(clf_2.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.3f}\".format(clf_2.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "_3BbXp7D-cpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Model"
      ],
      "metadata": {
        "id": "r-W8kv7oAS6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model (Convolutional Neural Network)"
      ],
      "metadata": {
        "id": "pVouYp2xH9Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense "
      ],
      "metadata": {
        "id": "WwaivsmJAx8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=256, kernel_size=10, activation='relu', input_shape=(X_train_tensor.shape[1], X_train_tensor.shape[2])))\n",
        "model.add(Conv1D(filters=128, kernel_size=10, activation='relu'))\n",
        "model.add(Conv1D(filters=64, kernel_size=10, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=8))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.get_feature_names_out()), activation='sigmoid'))"
      ],
      "metadata": {
        "id": "_OwW3VEHBXCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Sm6y1bdiC9PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "H_DBoRjaBsmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "E1KzYvHWIiMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "EPOCH = 25\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "model_history = model.fit(X_train_tensor, y_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_data=(X_test_tensor, y_test))"
      ],
      "metadata": {
        "id": "a7vwtRrsDEaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat fungsi untuk plotting hasil training\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "LEcjMdljDMTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(model_history, 'accuracy')\n",
        "plot_graphs(model_history, 'loss')"
      ],
      "metadata": {
        "id": "ZQNyK8cxEG6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nEpoch No.  Train Accuracy  Train Loss      Val Accuracy    Val Loss')\n",
        "for i in range(EPOCH):\n",
        "  print('{:8d} {:10f} \\t {:10f} \\t {:10f} \\t {:10f}'.format(i + 1, model_history.history['accuracy'][i], model_history.history['loss'][i], model_history.history['val_accuracy'][i], model_history.history['val_loss'][i]))"
      ],
      "metadata": {
        "id": "HegU1BaAEVat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "shF5bZvjIlUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lakukan prediksi pada data uji\n",
        "y_pred = np.argmax(model.predict(X_test_tensor), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test_tensor, y_test)"
      ],
      "metadata": {
        "id": "okb1ruAdEcIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Tampilkan laporan klasifikasi yang dilakukan model pada data uji\n",
        "print(classification_report(y_pred, y_true, target_names=label_encoder.get_feature_names_out([''])))"
      ],
      "metadata": {
        "id": "-pdjYXt7I0W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08 Prediction"
      ],
      "metadata": {
        "id": "6EKKj4HhKScH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data"
      ],
      "metadata": {
        "id": "qPpl0VCQLyTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_audio_path = sample_data\n",
        "\n",
        "ipd.Audio(sample_data)"
      ],
      "metadata": {
        "id": "EpU3p4z_JdPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_mfcc, demo_pitch, demo_mag, demo_chrom = get_audio_features(demo_audio_path)\n",
        "\n",
        "mfcc = pd.Series(demo_mfcc)\n",
        "pit = pd.Series(demo_pitch)\n",
        "mag = pd.Series(demo_mag)\n",
        "C = pd.Series(demo_chrom)\n",
        "\n",
        "demo_audio_features = pd.concat([mfcc,pit,mag,C], ignore_index=True)"
      ],
      "metadata": {
        "id": "dJ7F3XBTLQL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_audio_features"
      ],
      "metadata": {
        "id": "2sKhjDEALSGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape to 3D tensor\n",
        "demo_audio_features = np.expand_dims(demo_audio_features, axis=0)\n",
        "demo_audio_features = np.expand_dims(demo_audio_features, axis=2)\n",
        "\n",
        "demo_audio_features.shape"
      ],
      "metadata": {
        "id": "l25aCtJ4LYgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_preds = model.predict(demo_audio_features)\n",
        "demo_preds"
      ],
      "metadata": {
        "id": "Wt7TcucuLdrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = ['anger','disgust','fear','happy','neutral','sad','surprise']"
      ],
      "metadata": {
        "id": "_mUPWFZbLplh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = demo_preds.argmax(axis=1).item()\n",
        "index"
      ],
      "metadata": {
        "id": "Ma-TV7UGLjrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions[index]"
      ],
      "metadata": {
        "id": "EhirWRglLmAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 09 Simple Deployment Using Gradio"
      ],
      "metadata": {
        "id": "oejskgNbMsXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!\n",
        "https://gradio.app/\n",
        "'''\n",
        "\n",
        "!pip -q install gradio"
      ],
      "metadata": {
        "id": "PPSxg5JDMtV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def ser(file):\n",
        "  audio_mfcc, audio_pitch, audio_mag, audio_chrom = get_audio_features(file)\n",
        "\n",
        "  mfcc = pd.Series(audio_mfcc)\n",
        "  pit = pd.Series(audio_pitch)\n",
        "  mag = pd.Series(audio_mag)\n",
        "  C = pd.Series(audio_chrom)\n",
        "\n",
        "  audio_features = pd.concat([mfcc,pit,mag,C], ignore_index=True)\n",
        "\n",
        "  audio_features = np.expand_dims(audio_features, axis=0)\n",
        "  audio_features = np.expand_dims(audio_features, axis=2)\n",
        "\n",
        "  model_predict = model.predict(audio_features)\n",
        "  model_output = model_predict.argmax(axis=1).item()\n",
        "\n",
        "  emotions = ['anger','disgust','fear','happy','neutral','sad','surprise']\n",
        "  \n",
        "  return emotions[model_output]\n",
        "\n",
        "iface = gr.Interface(fn=ser, \n",
        "                     inputs=gr.inputs.Audio(source=\"microphone\", type=\"filepath\"), \n",
        "                     outputs=\"text\",\n",
        "                     live=True)\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "WYfqD_q1BOgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Student Activity"
      ],
      "metadata": {
        "id": "KMmNiXsKI2a0"
      }
    }
  ]
}